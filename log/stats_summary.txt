The log is parsing from triton_client.get_inference_statistics(), to better human readability. 
To learn more about the log, please refer to: 
1. https://github.com/triton-inference-server/server/blob/main/docs/user_guide/metrics.md 
2. https://github.com/triton-inference-server/server/issues/5374 

To better improve throughput, we always would like let requests wait in the queue for a while, and then execute them with a larger batch size. 
However, there is a trade-off between the increased queue time and the increased batch size. 
You may change 'max_queue_delay_microseconds' and 'preferred_batch_size' in the model configuration file to achieve this. 
See https://github.com/triton-inference-server/server/blob/main/docs/user_guide/model_configuration.md#delayed-batching for more details. 

model name is whisper 
queue time 12174.67 s, compute infer time 3842.61 s, compute input time 6.35  s, compute output time 2.96  s 
execuate inference with batch_size 1  total 60    times, total_infer_time 17968.68  ms, avg_infer_time 17968.68 /60   =299.48 ms, avg_infer_time_per_sample 17968.68 /60   /1=299.48 ms 
execuate inference with batch_size 2  total 17    times, total_infer_time 6473.81   ms, avg_infer_time 6473.81  /17   =380.81 ms, avg_infer_time_per_sample 6473.81  /17   /2=190.41 ms 
execuate inference with batch_size 3  total 14    times, total_infer_time 7354.65   ms, avg_infer_time 7354.65  /14   =525.33 ms, avg_infer_time_per_sample 7354.65  /14   /3=175.11 ms 
execuate inference with batch_size 4  total 7     times, total_infer_time 3043.52   ms, avg_infer_time 3043.52  /7    =434.79 ms, avg_infer_time_per_sample 3043.52  /7    /4=108.70 ms 
execuate inference with batch_size 5  total 2     times, total_infer_time 1072.73   ms, avg_infer_time 1072.73  /2    =536.36 ms, avg_infer_time_per_sample 1072.73  /2    /5=107.27 ms 
execuate inference with batch_size 6  total 4     times, total_infer_time 2293.99   ms, avg_infer_time 2293.99  /4    =573.50 ms, avg_infer_time_per_sample 2293.99  /4    /6=95.58 ms 
execuate inference with batch_size 7  total 3     times, total_infer_time 1925.27   ms, avg_infer_time 1925.27  /3    =641.76 ms, avg_infer_time_per_sample 1925.27  /3    /7=91.68 ms 
execuate inference with batch_size 8  total 659   times, total_infer_time 468106.49 ms, avg_infer_time 468106.49/659  =710.33 ms, avg_infer_time_per_sample 468106.49/659  /8=88.79 ms 
